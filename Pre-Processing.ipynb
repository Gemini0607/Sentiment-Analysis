{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from textblob import TextBlob\n",
    "import glob\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the stopwords file from the NLTK package\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['\\'nt','\\'s','n\\'t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the file names for the positive and negative labelled text data\n",
    "\n",
    "# Negative File Names\n",
    "files_neg = glob.glob('C:/Users/Mitesh/Desktop/IPython/TEXT DATASETS/3/review_polarity/txt_sentoken/neg/*.txt')\n",
    "\n",
    "# Positive File Names\n",
    "files_pos = glob.glob('C:/Users/Mitesh/Desktop/IPython/TEXT DATASETS/3/review_polarity/txt_sentoken/pos/*.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for reading the actual textual data from the data corpus\n",
    "def in_text(file_names):\n",
    "    data = []\n",
    "    for i in file_names:\n",
    "        with open(i,'r') as fn:\n",
    "            text = fn.read()\n",
    "            data.append(text)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Reading in textual data for the positive and negative files respectively.\n",
    "\n",
    "# Positive\n",
    "pos = in_text(files_pos)\n",
    "print len(pos)\n",
    "\n",
    "# Negative\n",
    "neg = in_text(files_neg)\n",
    "print len(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# List of numerical & special characters to be removed from the corpus\n",
    "nos = list(range(10))\n",
    "nos = [str(x) for x in nos]\n",
    "\n",
    "l = '!\"#$%&()*+,-/:;<=>?@[\\]^_`{|}~'\n",
    "ls = []\n",
    "\n",
    "for i in l:\n",
    "    ls.append(i)\n",
    "\n",
    "sp = nos + ls\n",
    "print sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to Remove Special Characters and Store the obtained processed data in List format\n",
    "\n",
    "def rmv(lst):\n",
    "    test = []\n",
    "    for txt in lst:\n",
    "        for letter in txt:\n",
    "            if (letter in sp):\n",
    "                txt = txt.replace(letter,'')\n",
    "        test.append(txt)\n",
    "    return test\n",
    "\n",
    "\n",
    "def rm_spch(s):\n",
    "    s = rmv(s)\n",
    "    one = []\n",
    "    x = ''\n",
    "    \n",
    "    for num in s:\n",
    "        x = str(num)\n",
    "        test = word_tokenize(x)\n",
    "        \n",
    "        res = test[:]\n",
    "\n",
    "        for xn in test:          \n",
    "            if (xn in  stop_words):\n",
    "                    res.remove(xn)\n",
    "                \n",
    "        sen = ' '.join(res)\n",
    "        one.append(sen)\n",
    "    return one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positive Labelled Texts: 1000\n",
      "Number of Negative Labelled Texts: 1000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# Pre-Precessing the data set and storing it in as semi - processed text file\n",
    "\n",
    "refined_pos = rm_spch(pos)\n",
    "print 'Number of Positive Labelled Texts:',len(refined_pos)\n",
    "\n",
    "refined_neg = rm_spch(neg)\n",
    "print 'Number of Negative Labelled Texts:',len(refined_neg)\n",
    "\n",
    "one = refined_pos + refined_neg\n",
    "print len(one)\n",
    "\n",
    "with open('data.txt','w') as dr:\n",
    "    for i in one:\n",
    "        dr.write(i+'|')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting the corpus to 'textblob' objects \n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "# Positive\n",
    "txt_pos = TextBlob(str(refined_pos), analyzer = NaiveBayesAnalyzer())\n",
    "\n",
    "# Negative\n",
    "\n",
    "txt_neg = TextBlob(str(refined_neg), analyzer = NaiveBayesAnalyzer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences in Positive Labelled Texts: 33714\n",
      "Number of Sentences in Negative Labelled Texts: 32162\n",
      "2713353\n",
      "2387331\n"
     ]
    }
   ],
   "source": [
    "# A measure of the total number of sentences contained in both the positive and negative tagged files\n",
    "\n",
    "print 'Number of Sentences in Positive Labelled Texts:',len(txt_pos.sentences)\n",
    "print 'Number of Sentences in Negative Labelled Texts:',len(txt_neg.sentences)\n",
    "print len(txt_pos)\n",
    "print len(txt_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Sentences: 31818\n",
      "Individual Sentences: 29533\n"
     ]
    }
   ],
   "source": [
    "# In order to remove any text that may be of lesser or no value due to earlier pre-processing, we set a minimum length for \n",
    "# the text; this value is not absolute and can be varied.\n",
    "\n",
    "#Positive\n",
    "sen_pos = txt_pos.sentences \n",
    "\n",
    "for i in sen_pos:      \n",
    "    if len(i) <= 15:\n",
    "        sen_pos.remove(i)\n",
    "                  \n",
    "print 'Individual Sentences:',len(sen_pos)\n",
    "\n",
    "# Negative\n",
    "sen_neg = txt_neg.sentences\n",
    "\n",
    "for i in sen_neg:\n",
    "    if len(i) <= 15:\n",
    "        sen_neg.remove(i)\n",
    "    \n",
    "print 'Individual Sentences:',len(sen_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61351\n"
     ]
    }
   ],
   "source": [
    "# Summing up all the texts in one single List object\n",
    "total = [x for x in sen_pos] + [y for y in sen_neg]\n",
    "print len(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos    37484\n",
      "neg    22352\n",
      "Name: Polarity, dtype: int64\n",
      "59836\n"
     ]
    }
   ],
   "source": [
    "# Creating a Dictionay comprising of each individual Text and its respective Polarity and saving it as a DataFarme\n",
    "\n",
    "di = {'Text':total, 'Polarity':[z.sentiment[0] for z in total]}\n",
    "df = DataFrame(di)\n",
    "\n",
    "df = df[df.Text != '.']   # Some more processing to get rid of the '.' sentences\n",
    "\n",
    "print df.Polarity.value_counts()\n",
    "\n",
    "print len(df)\n",
    "\n",
    "df.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
